import os
import sys

import plotly.graph_objects as go
import streamlit as st
import pandas as pd

from src.qa_system import AutomotiveQASystem
from src.retrieval.multi_retriever import MultiPathRetriever
from src.utils.text_processor import VectorStoreManager

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config import config, Config
from src.utils.data_loader import AutomotiveLoader


class AutomotiveQAApp:
    """æ±½è½¦è¡Œä¸šé—®ç­”åº”ç”¨"""

    def __init__(self):
        self.setup_page()
        self.init_session_state()
        # æ‰‹åŠ¨è¿æ¥

    def setup_page(self):
        """è®¾ç½®é¡µé¢é…ç½®"""
        st.set_page_config(
            page_title="æ±½è½¦è¡Œä¸šä¸“ä¸šçŸ¥è¯†é—®ç­”ç³»ç»Ÿ",
            page_icon="ğŸš—",
            layout="wide",
            initial_sidebar_state="expanded"
        )
        st.title("ğŸš— æ±½è½¦è¡Œä¸šä¸“ä¸šçŸ¥è¯†é—®ç­”ç³»ç»Ÿ")
        st.markdown("åŸºäºRAGæŠ€æœ¯çš„ä¸“ä¸šæ±½è½¦çŸ¥è¯†é—®ç­”å¹³å°")

    def init_session_state(self):
        """åˆå§‹åŒ–ä¼šè¯çŠ¶æ€"""
        if 'qa_system' not in st.session_state:
            st.session_state.qa_system = None
        if 'vector_manager' not in st.session_state:
            st.session_state.vector_manager = None
        if 'knowledge_loaded' not in st.session_state:
            st.session_state.knowledge_loaded = False
        if 'chat_history' not in st.session_state:
            st.session_state.chat_history = []
        if 'comparison_results' not in st.session_state:
            st.session_state.comparison_results = []

    def load_knowledge_base(self):
        """åŠ è½½çŸ¥è¯†åº“"""
        st.sidebar.header("ğŸ“š çŸ¥è¯†åº“ç®¡ç†")
        if st.sidebar.button("ğŸ”„ åˆå§‹åŒ–çŸ¥è¯†åº“", type="primary"):
            with st.spinner("æ­£åœ¨åŠ è½½çŸ¥è¯†åº“ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ..."):
                try:
                    # åŠ è½½æ–‡æ¡£
                    data_loader = AutomotiveLoader(config.RAW_DATA_DIR)
                    documents = data_loader.load_data()

                    # åˆ›å»ºå‘é‡æ•°æ®åº“
                    vector_store = VectorStoreManager(persist_db_dir=config.VECTOR_DB_PATH, persist_storage_dir=config.VECTOR_STORAGE_PATH)
                    index = vector_store.add_documents_to_collection(
                        collection_name=config.COLLECTION_NAME,
                        documents=documents,
                    )
                    # åˆå§‹åŒ–æ£€ç´¢å™¨å’Œé—®ç­”ç³»ç»Ÿ
                    multi_retriever = MultiPathRetriever(vector_store, vector_store.get_corpus(config.COLLECTION_NAME))
                    qa_system = AutomotiveQASystem(
                        retriever=multi_retriever,
                        model_type=Config.MODEL_TYPE_QWEN,
                        model_name=Config.QWEN3_MAX_MODEL
                    )
                    # ä¿å­˜åˆ°ä¼šè¯çŠ¶æ€
                    st.session_state.vector_store = vector_store
                    st.session_state.qa_system = qa_system
                    st.session_state.knowledge_loaded = True
                    st.success(f"âœ… çŸ¥è¯†åº“åŠ è½½å®Œæˆï¼å…±å¤„ç† {len(documents)} ä¸ªæ–‡æ¡£ï¼Œ{len(index.storage_context.docstore.docs)} ä¸ªæ–‡æœ¬å—")
                except Exception as e:
                    st.error(f"åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {str(e)}")

    def chat_interface(self):
        """èŠå¤©ç•Œé¢"""
        st.header("ğŸ’¬ ä¸“ä¸šé—®ç­”")

        # æ˜¾ç¤ºèŠå¤©å†å²
        for chat in st.session_state.chat_history:
            with st.chat_message("user"):
                st.write(chat["question"])
            with st.chat_message("assistant"):
                st.write(chat["answer"])
                if chat.get("confidence"):
                    st.progress(chat["confidence"], text=f"ç½®ä¿¡åº¦: {chat['confidence']:.2%}")

        # ç”¨æˆ·è¾“å…¥
        if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„æ±½è½¦è¡Œä¸šé—®é¢˜..."):
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            with st.chat_message("user"):
                st.write(prompt)

            # ç”Ÿæˆå›ç­”
            if st.session_state.qa_system:
                with st.chat_message("assistant"):
                    with st.spinner("æ­£åœ¨æ£€ç´¢ä¿¡æ¯å¹¶ç”Ÿæˆå›ç­”..."):
                        result = st.session_state.qa_system.generate_answer(prompt)

                        st.write(result["answer"])
                        st.progress(result["confidence"], text=f"ç½®ä¿¡åº¦: {result['confidence']:.2%}")

                        # æ˜¾ç¤ºæ£€ç´¢ä¿¡æ¯
                        with st.expander("ğŸ“„ æŸ¥çœ‹æ£€ç´¢åˆ°çš„å‚è€ƒæ–‡æ¡£"):
                            for i, (doc, score) in enumerate(result["retrieved_documents"]):
                                st.markdown(f"**æ–‡æ¡£ {i + 1} (ç›¸å…³åº¦: {score:.3f})**")
                                st.text(doc[:200] + "..." if len(doc) > 200 else doc)
                                st.divider()

                # ä¿å­˜åˆ°èŠå¤©å†å²
                st.session_state.chat_history.append({
                    "question": prompt,
                    "answer": result["answer"],
                    "confidence": result["confidence"]
                })
            else:
                st.error("è¯·å…ˆåˆå§‹åŒ–çŸ¥è¯†åº“")

    def generate_direct_answer(self, question: str) -> str:
        """ç”Ÿæˆæ— RAGçš„ç›´æ¥å›ç­”"""
        try:
            # è¿™é‡Œå®ç°ç›´æ¥è°ƒç”¨LLMçš„é€»è¾‘ï¼Œä¸ä½¿ç”¨æ£€ç´¢
            # ä½¿ç”¨ç›¸åŒçš„LLMï¼Œä½†ä¸æä¾›æ£€ç´¢çš„ä¸Šä¸‹æ–‡
            from langchain_core.messages import HumanMessage, SystemMessage

            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ±½è½¦è¡Œä¸šä¸“å®¶åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä½ çš„çŸ¥è¯†å‡†ç¡®ã€ä¸“ä¸šåœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

            å›ç­”è¦æ±‚ï¼š
            1. ä½¿ç”¨ä¸“ä¸šã€å‡†ç¡®çš„æ±½è½¦è¡Œä¸šæœ¯è¯­
            2. å›ç­”è¦ç»“æ„æ¸…æ™°ï¼Œé‡ç‚¹çªå‡º
            3. å¦‚æœæ¶‰åŠæŠ€æœ¯å‚æ•°æˆ–è§„æ ¼ï¼Œè¯·ç¡®ä¿æ•°æ®å‡†ç¡®
            4. å¦‚æœä¸çŸ¥é“ç¡®åˆ‡ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜

            è¯·å¼€å§‹å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š"""

            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=f"ç”¨æˆ·é—®é¢˜: {question}")
            ]

            response = st.session_state.qa_system.llm.invoke(messages)
            return response.content
        except Exception as e:
            return f"ç”Ÿæˆç›´æ¥å›ç­”æ—¶å‡ºé”™: {str(e)}"

    def compare_rag_vs_direct(self, question: str):
        """å¯¹æ¯”æœ‰RAGå’Œæ— RAGçš„å›ç­”"""
        if not st.session_state.qa_system:
            st.error("è¯·å…ˆåˆå§‹åŒ–çŸ¥è¯†åº“")
            return

        with st.spinner("æ­£åœ¨ç”Ÿæˆå¯¹æ¯”åˆ†æ..."):
            # æœ‰RAGçš„å›ç­”
            rag_result = st.session_state.qa_system.generate_answer(question)

            # æ— RAGçš„å›ç­”
            direct_answer = self.generate_direct_answer(question)

            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
            rag_answer_length = len(rag_result["answer"])
            direct_answer_length = len(direct_answer)

            # è¯„ä¼°æŒ‡æ ‡ï¼ˆè¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„è¯„ä¼°é€»è¾‘ï¼‰
            rag_score = self.evaluate_answer_quality(rag_result["answer"], question)
            direct_score = self.evaluate_answer_quality(direct_answer, question)

            # ä¿å­˜å¯¹æ¯”ç»“æœ
            comparison_result = {
                "question": question,
                "rag_answer": rag_result["answer"],
                "direct_answer": direct_answer,
                "rag_confidence": rag_result["confidence"],
                "rag_retrieved_docs": len(rag_result["retrieved_documents"]),
                "rag_answer_length": rag_answer_length,
                "direct_answer_length": direct_answer_length,
                "rag_quality_score": rag_score,
                "direct_quality_score": direct_score
            }

            st.session_state.comparison_results.append(comparison_result)

            return comparison_result

    def evaluate_answer_quality(self, answer: str, question: str) -> float:
        """è¯„ä¼°å›ç­”è´¨é‡ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„è´¨é‡è¯„ä¼°é€»è¾‘
        # ç›®å‰ä½¿ç”¨ç®€å•çš„å¯å‘å¼è§„åˆ™

        score = 0.5  # åŸºç¡€åˆ†

        # æ ¹æ®å›ç­”é•¿åº¦åŠ åˆ†
        if len(answer) > 100:
            score += 0.1
        if len(answer) > 200:
            score += 0.1

        # æ ¹æ®ä¸“ä¸šæœ¯è¯­åŠ åˆ†
        professional_terms = ["å‘åŠ¨æœº", "å˜é€Ÿç®±", "åº•ç›˜", "æ‚¬æŒ‚", "åˆ¶åŠ¨", "æ–°èƒ½æº", "ç”µåŠ¨è½¦", "æ··åŠ¨"]
        found_terms = sum(1 for term in professional_terms if term in answer)
        score += min(found_terms * 0.05, 0.2)

        # æ ¹æ®ç»“æ„åŠ åˆ†ï¼ˆåŒ…å«æ•°å­—ã€åˆ—è¡¨ç­‰ï¼‰
        if any(char.isdigit() for char in answer):
            score += 0.1
        if "ã€" in answer or ";" in answer or "ï¼š" in answer:
            score += 0.1

        return min(score, 1.0)

    def evaluation_interface(self):
        """è¯„ä¼°ç•Œé¢"""
        st.header("ğŸ“Š ç³»ç»Ÿè¯„ä¼°")

        # ä½¿ç”¨é€‰é¡¹å¡ç»„ç»‡ä¸åŒçš„è¯„ä¼°åŠŸèƒ½
        tab1, tab2, tab3 = st.tabs(["ğŸ” æ£€ç´¢æ•ˆæœæµ‹è¯•", "ğŸ”„ RAGå¯¹æ¯”æµ‹è¯•", "ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡"])

        with tab1:
            col1, col2 = st.columns(2)
            with col1:
                st.subheader("æ£€ç´¢æ•ˆæœæµ‹è¯•")
                test_question = st.text_input("æµ‹è¯•é—®é¢˜", key="retrieval_test")
                if st.button("æµ‹è¯•æ£€ç´¢æ•ˆæœ"):
                    if st.session_state.qa_system:
                        result = st.session_state.qa_system.generate_answer(test_question)
                        # æ˜¾ç¤ºæ£€ç´¢ç»“æœåˆ†æ
                        if result["retrieved_documents"]:
                            fig = go.Figure(data=[
                                go.Bar(name='ç›¸å…³åº¦åˆ†æ•°',
                                       x=[f"æ–‡æ¡£{i + 1}" for i in range(len(result["retrieved_documents"]))],
                                       y=[score for _, score in result["retrieved_documents"]])
                            ])
                            fig.update_layout(title="æ£€ç´¢æ–‡æ¡£ç›¸å…³åº¦åˆ†å¸ƒ")
                            st.plotly_chart(fig)
                        else:
                            st.warning("æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£")

            with col2:
                st.subheader("æ€§èƒ½æŒ‡æ ‡")
                if st.button("ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"):
                    # è¿™é‡Œå¯ä»¥æ·»åŠ è‡ªåŠ¨è¯„ä¼°é€»è¾‘
                    st.info("è¯„ä¼°åŠŸèƒ½å¼€å‘ä¸­...")

        with tab2:
            st.subheader("RAGä¸æ— RAGå¯¹æ¯”æµ‹è¯•")

            # æµ‹è¯•é—®é¢˜è¾“å…¥
            col1, col2 = st.columns([3, 1])
            with col1:
                compare_question = st.text_input("è¾“å…¥å¯¹æ¯”æµ‹è¯•é—®é¢˜",
                                                 placeholder="è¯·è¾“å…¥ä¸€ä¸ªæ±½è½¦è¡Œä¸šç›¸å…³çš„é—®é¢˜...",
                                                 key="compare_question")
            with col2:
                st.write("")  # å ä½
                st.write("")
                run_comparison = st.button("ğŸš€ è¿è¡Œå¯¹æ¯”æµ‹è¯•", width='stretch')

            if run_comparison and compare_question:
                result = self.compare_rag_vs_direct(compare_question)

                if result:
                    # æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
                    st.success("å¯¹æ¯”æµ‹è¯•å®Œæˆï¼")

                    # ä½¿ç”¨åˆ—å¸ƒå±€å±•ç¤ºç»“æœ
                    col1, col2 = st.columns(2)

                    with col1:
                        st.markdown("### ğŸ”— æœ‰RAGå›ç­”")
                        st.info(f"**ç½®ä¿¡åº¦:** {result['rag_confidence']:.3f}")
                        st.info(f"**æ£€ç´¢æ–‡æ¡£æ•°:** {result['rag_retrieved_docs']}")
                        st.info(f"**å›ç­”è´¨é‡åˆ†:** {result['rag_quality_score']:.3f}")
                        st.markdown("**å›ç­”å†…å®¹:**")
                        st.write(result["rag_answer"])

                        with st.expander("æŸ¥çœ‹æ£€ç´¢æ–‡æ¡£"):
                            rag_full_result = st.session_state.qa_system.generate_answer(compare_question)
                            for i, (doc, score) in enumerate(rag_full_result["retrieved_documents"]):
                                st.markdown(f"**æ–‡æ¡£ {i + 1} (ç›¸å…³åº¦: {score:.3f})**")
                                st.text(doc[:300] + "..." if len(doc) > 300 else doc)
                                st.divider()

                    with col2:
                        st.markdown("### ğŸ¯ æ— RAGå›ç­”")
                        st.info(f"**å›ç­”è´¨é‡åˆ†:** {result['direct_quality_score']:.3f}")
                        st.info(f"**å›ç­”é•¿åº¦:** {result['direct_answer_length']} å­—ç¬¦")
                        st.markdown("**å›ç­”å†…å®¹:**")
                        st.write(result["direct_answer"])

                    # æ˜¾ç¤ºè¯„ä¼°æŒ‡æ ‡å¯¹æ¯”
                    st.markdown("---")
                    st.subheader("ğŸ“Š è¯„ä¼°æŒ‡æ ‡å¯¹æ¯”")

                    metrics_data = {
                        "æŒ‡æ ‡": ["ç½®ä¿¡åº¦", "å›ç­”è´¨é‡", "å›ç­”é•¿åº¦", "æ£€ç´¢æ–‡æ¡£æ•°"],
                        "æœ‰RAG": [
                            f"{result['rag_confidence']:.3f}",
                            f"{result['rag_quality_score']:.3f}",
                            f"{result['rag_answer_length']}",
                            f"{result['rag_retrieved_docs']}"
                        ],
                        "æ— RAG": [
                            "N/A",
                            f"{result['direct_quality_score']:.3f}",
                            f"{result['direct_answer_length']}",
                            "N/A"
                        ]
                    }

                    metrics_df = pd.DataFrame(metrics_data)
                    st.dataframe(metrics_df,  width='stretch', hide_index=True)

                    # è´¨é‡åˆ†æ•°å¯¹æ¯”å›¾
                    quality_data = {
                        "ç±»å‹": ["æœ‰RAG", "æ— RAG"],
                        "è´¨é‡åˆ†æ•°": [result['rag_quality_score'], result['direct_quality_score']]
                    }
                    quality_df = pd.DataFrame(quality_data)

                    fig = go.Figure(data=[
                        go.Bar(name='è´¨é‡åˆ†æ•°', x=quality_df['ç±»å‹'], y=quality_df['è´¨é‡åˆ†æ•°'])
                    ])
                    fig.update_layout(title="å›ç­”è´¨é‡å¯¹æ¯”", yaxis_range=[0, 1])
                    st.plotly_chart(fig)

            # æ˜¾ç¤ºå†å²å¯¹æ¯”è®°å½•
            if st.session_state.comparison_results:
                st.markdown("---")
                st.subheader("å†å²å¯¹æ¯”è®°å½•")

                history_data = []
                for i, comp in enumerate(st.session_state.comparison_results[-5:]):  # åªæ˜¾ç¤ºæœ€è¿‘5æ¡
                    history_data.append({
                        "åºå·": i + 1,
                        "é—®é¢˜": comp["question"][:50] + "..." if len(comp["question"]) > 50 else comp["question"],
                        "RAGè´¨é‡åˆ†": f"{comp['rag_quality_score']:.3f}",
                        "æ— RAGè´¨é‡åˆ†": f"{comp['direct_quality_score']:.3f}",
                        "è´¨é‡å·®å¼‚": f"{(comp['rag_quality_score'] - comp['direct_quality_score']):.3f}"
                    })

                if history_data:
                    history_df = pd.DataFrame(history_data)
                    st.dataframe(history_df,  width='stretch', hide_index=True)

                    # æ¸…ç©ºå†å²è®°å½•æŒ‰é’®
                    if st.button("æ¸…ç©ºå¯¹æ¯”è®°å½•"):
                        st.session_state.comparison_results = []
                        st.rerun()

        with tab3:
            st.subheader("ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡")
            if st.button("ç”Ÿæˆè¯¦ç»†è¯„ä¼°æŠ¥å‘Š"):
                # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å…¨é¢çš„è¯„ä¼°é€»è¾‘
                st.info("è¯¦ç»†è¯„ä¼°æŠ¥å‘ŠåŠŸèƒ½å¼€å‘ä¸­...")

                # æ¨¡æ‹Ÿä¸€äº›æ€§èƒ½æŒ‡æ ‡
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("å¹³å‡å“åº”æ—¶é—´", "2.3s", "-0.2s")
                with col2:
                    st.metric("æ£€ç´¢å‡†ç¡®ç‡", "87%", "3%")
                with col3:
                    st.metric("ç”¨æˆ·æ»¡æ„åº¦", "92%", "5%")

    def run(self):
        """è¿è¡Œåº”ç”¨"""
        # ä¾§è¾¹æ 
        self.load_knowledge_base()
        # ä¸»ç•Œé¢æ ‡ç­¾é¡µ
        tab1, tab2, tab3 = st.tabs(["ğŸ’¬ é—®ç­”", "ğŸ“Š è¯„ä¼°", "â„¹ï¸ å…³äº"])
        with tab1:
            self.chat_interface()
        with tab2:
            self.evaluation_interface()
        with tab3:
            st.header("å…³äºæœ¬é¡¹ç›®")
            # st.markdown("""
            # ### åŸºäºRAGçš„æ±½è½¦è¡Œä¸šä¸“ä¸šçŸ¥è¯†é—®ç­”ç³»ç»Ÿ
            #
            # **æŠ€æœ¯æ ˆ:**
            # - LangChain: æ¡†æ¶é›†æˆ
            # - Chroma: å‘é‡æ•°æ®åº“
            # - Sentence Transformers: æ–‡æœ¬åµŒå…¥
            # - OpenAI GPT: ç­”æ¡ˆç”Ÿæˆ
            # - Streamlit: Webç•Œé¢
            #
            # **æ ¸å¿ƒåŠŸèƒ½:**
            # - å¤šè·¯æ£€ç´¢ï¼ˆå¯†é›†æ£€ç´¢ + å…³é”®è¯æ£€ç´¢ï¼‰
            # - é‡æ’åºä¼˜åŒ–
            # - ç½®ä¿¡åº¦è¯„ä¼°
            # - å®æ—¶é—®ç­”
            #
            # **æ•°æ®æº:**
            # - æ±½è½¦æŠ€æœ¯æ‰‹å†Œ
            # - ç»´ä¿®æŒ‡å—
            # - è¡Œä¸šæŠ¥å‘Š
            # - æŠ€æœ¯æ–‡æ¡£
            # """)


if __name__ == "__main__":
    app = AutomotiveQAApp()
    app.run()